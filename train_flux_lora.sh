/home/1000/.local/bin/accelerate launch \
  --dynamo_backend=tensorrt \
  --dynamo_mode=default \
  --mixed_precision=fp16 \
  --num_processes=1 \
  --num_machines=1 \
  --num_cpu_threads_per_process=2 \
  /app/sd-scripts/flux_train_network.py \
  --ae=/app/models/vae/ae.safetensors \
  --bucket_no_upscale \
  --bucket_reso_steps=64 \
  --cache_latents \
  --cache_latents_to_disk \
  --caption_extension=".txt" \
  --clip_l=/app/models/clip/clip_l.safetensors \
  --clip_skip=1 \
  --discrete_flow_shift=3.0 \
  --enable_bucket \
  --gradient_accumulation_steps=1 \
  --guidance_scale=3.5 \
  --huber_c=0.1 \
  --huber_scale=1 \
  --huber_schedule=snr \
  --logging_dir=/app/logs \
  --loss_type=l2 \
  --lr_scheduler=cosine_with_min_lr  \
  --lr_scheduler_num_cycles=2 \
  --lr_scheduler_power=1 \
  --lr_warmup_steps=0.1 \
  --lr_decay_steps=0.5 \
  --lr_scheduler_min_lr_ratio=0.1 \
  --max_bucket_reso=2048 \
  --max_data_loader_n_workers=0 \
  --max_grad_norm=1 \
  --max_timestep=1000 \
  --max_train_epochs=10 \
  --max_train_steps=10000 \
  --min_bucket_reso=256 \
  --mixed_precision=bf16 \
  --model_prediction_type=raw \
  --network_alpha=32 \
  --network_args="train_double_block_indices=all" \
  --network_args="train_single_block_indices=all" \
  --network_dim=32 \
  --network_module=networks.lora_flux \
  --network_train_unet_only \
  --optimizer_type=PagedAdamW8bit \
  --optimizer_args weight_decay=0.01 betas=0.9,0.95 \
  --output_dir=/app/outputs \
  --output_name=qili \
  --pretrained_model_name_or_path=/app/models/unet/flux1-dev-fp8.safetensors \
  --prior_loss_weight=1 \
  --resolution=1024,1024 \
  --sample_prompts=/app/outputs/sample/prompt.txt \
  --sample_sampler=euler_a \
  --save_every_n_epochs=1 \
  --save_model_as=safetensors \
  --save_precision=bf16 \
  --t5xxl=/app/models/clip/t5xxl_fp16.safetensors \
  --t5xxl_max_token_length=512 \
  --timestep_sampling=sigma \
  --train_batch_size=2 \
  --train_data_dir=/app/data/img \
  --unet_lr=0.0001 \
  --wandb_run_name=qili \
  --xformers